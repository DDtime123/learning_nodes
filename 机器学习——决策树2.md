# 对树的节点建模

~~~python
class Node:
    """Contains the information of the node and another nodes of the Decision Tree."""

    def __init__(self):
        self.value = None
        self.next = None
        self.childs = None
~~~

# 决策树分类器类

~~~python
class DecisionTreeClassifier:
    """Decision Tree Classifier using ID3 algorithm."""

    def __init__(self, X, feature_names, labels):
        self.X = X  # features or predictors
        self.feature_names = feature_names  # name of the features
        self.labels = labels  # categories
        self.labelCategories = list(set(labels))  # unique categories
        # number of instances of each category
        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]
        self.node = None  # nodes
        # calculate the initial entropy of the system
        self.entropy = self._get_entropy([x for x in range(len(self.labels))])
~~~

 为了计算熵注，我们使用私有函数 _get_entropy()。该函数的代码如下： 

~~~python
def _get_entropy(self, x_ids):
    """ Calculates the entropy.
    Parameters
    __________
    :param x_ids: list, List containing the instances ID's
    __________
    :return: entropy: float, Entropy.
    """
    # sorted labels by instance id
    labels = [self.labels[i] for i in x_ids]
    # count number of instances of each category
    label_count = [labels.count(x) for x in self.labelCategories]
    # calculate the entropy for each category and sum them
    entropy = sum([-count / len(x_ids) * math.log(count / len(x_ids), 2)
                   if count else 0
                   for count in label_count
                  ])
        
    return entropy
~~~

 我们将实例 ID 或索引传递给这个函数。为此，我们需要为每个实例生成一个唯一编号。 



 我们将编写一个 ID3 算法，该算法使用**信息增益**来找到最大化它的特征并基于该特征进行分割。信息增益基于熵。 

 **熵**是对随机变量不确定性的度量。如果我们最小化熵，那么我们就增加了变量的确定性。换句话说，如果随机变量只能取一个值，则熵达到最小值，而如果所有值都是等概率的，则熵最大。 

 该算法将尝试最小化熵，或者等效地最大化信息增益。

 我们可以使用以下公式计算熵： 

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211105190500.png)

其中*pi*是每个类别 i=1,…,N 的比例。

我们说过我们会计算信息增益来选择最大化它的特征，然后根据该特征进行分割。信息增益是一个基于熵的概念。如果我们选择一个特定的特征 j，它被定义为总熵减去熵。

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211105190546.png)

 所以我们创建了另一个**计算信息增益**的私有函数： 

~~~python
def _get_information_gain(self, x_ids, feature_id):
    """Calculates the information gain for a given feature based on its entropy and the total entropy of the system.
    Parameters
    __________
    :param x_ids: list, List containing the instances ID's
    :param feature_id: int, feature ID
    __________
    :return: info_gain: float, the information gain for a given feature.
    """
    # calculate total entropy
    info_gain = self._get_entropy(x_ids)
    # store in a list all the values of the chosen feature
    x_features = [self.X[x][feature_id] for x in x_ids]
    # get unique values
    feature_vals = list(set(x_features))
    # get frequency of each value
    feature_v_count = [x_features.count(x) for x in feature_vals]
    # get the feature values ids
    feature_v_id = [
        [x_ids[i]
        for i, x in enumerate(x_features)
        if x == y]
        for y in feature_vals
    ]

    # compute the information gain with the chosen feature
    info_gain_feature = sum([v_counts / len(x_ids) * self._get_entropy(v_ids)
                        for v_counts, v_ids in zip(feature_v_count, feature_v_id)])

    info_gain = info_gain - info_gain_feature

    return info_gain
~~~

 另一个函数来**找到最大化信息增益的特征** 

~~~python
def _get_feature_max_information_gain(self, x_ids, feature_ids):
    """Finds the attribute/feature that maximizes the information gain.
    Parameters
    __________
    :param x_ids: list, List containing the samples ID's
    :param feature_ids: list, List containing the feature ID's
    __________
    :returns: string and int, feature and feature id of the feature that maximizes the information gain
    """
    # get the entropy for each feature
    features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]
    # find the feature that maximises the information gain
    max_id = feature_ids[features_entropy.index(max(features_entropy))]

    return self.feature_names[max_id], max_id
~~~

 我们有一个私有函数（**_get_feature_max_information_gain（）** ），其发现最大化信息增益，使用另一个私有函数（功能**_get_information_gain（）** ）计算的信息增益，使用另一个私有函数（**_get_entropy（）** ）计算熵。



  初始化算法，然后使用私有函数递归调用算法来构建我们的树

~~~python
def id3(self):
    """Initializes ID3 algorithm to build a Decision Tree Classifier.
    :return: None
    """
    # assign an unique number to each instance
    x_ids = [x for x in range(len(self.X))]
    # assign an unique number to each featuer
    feature_ids = [x for x in range(len(self.feature_names))]
    # define node variable - instance of the class Node
    self.node = self._id3_recv(x_ids, feature_ids, self.node)
~~~



读取数据

~~~python
import pandas as pd
import numpy as np
# define features and target values
data = pd.read_csv('watermelons.csv')
data.head()
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211105191442.png)

# _id3_recv() 函数

现在我们有一些数据可以处理，理解 _id3_recv ( ) 函数会更容易。下面是带有一些注释的代码：

~~~python
def _id3_recv(self, x_ids, feature_ids, node):
    """ID3 algorithm. It is called recursively until some criteria is met.
    Parameters
    __________
    :param x_ids: list, list containing the samples ID's
    :param feature_ids: list, List containing the feature ID's
    :param node: object, An instance of the class Nodes
    __________
    :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree
    """
    if not node:
        node = Node()  # initialize nodes
    # sorted labels by instance id
    labels_in_features = [self.labels[x] for x in x_ids]
    # if all the example have the same class (pure node), return node
    if len(set(labels_in_features)) == 1:
        node.value = self.labels[x_ids[0]]
        return node
    # if there are not more feature to compute, return node with the most probable class
    if len(feature_ids) == 0:
        node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode
        return node
    # else...
    # choose the feature that maximizes the information gain
    best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)
    node.value = best_feature_name
    node.childs = []
    # value of the chosen feature for each instance
    feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))
    # loop through all the values
    for value in feature_values:
        child = Node()
        child.value = value  # add a branch from the node to each feature value in our feature
        node.childs.append(child)  # append new child node to current node
        child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]
        if not child_x_ids:
            child.next = max(set(labels_in_features), key=labels_in_features.count)
            print('')
        else:
            if feature_ids and best_feature_id in feature_ids:
                to_remove = feature_ids.index(best_feature_id)
                feature_ids.pop(to_remove)
            # recursively call the algorithm
            child.next = self._id3_recv(child_x_ids, feature_ids, child.next)
    return node
~~~



该函数采用两个包含实例 id 和特征 id 的列表以及类 Node.js 的一个实例。首先，它检查节点属性是否为*None*，如果是，则初始化类 Node 的实例。

然后，在每次拆分时，算法检查是否满足以下某些条件：

- **所有实例都属于同一类别吗？**如果是，它将类别分配给节点值并返回变量节点。以下代码块在函数中执行此任务：

~~~python
# if all the example have the same class (pure node), return node
      if len(set(labels_in_features)) == 1:
          node.value = self.labels[x_ids[0]]
          return node
~~~



- **是否有更多的特征来计算信息增益？**如果没有，它将最可能的类别分配给节点值（它计算模式）并返回变量节点。执行此操作的代码如下：

~~~python
 # if there are not more feature to compute, return node with the most probable class
        if len(feature_ids) == 0:
            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode
            return node
~~~

如果以上条件都不满足，则选择使信息增益最大化的特征并将其存储在节点值中。在这种情况下，节点值是**特征**与它使分裂。

ID3 算法为所选特征的每个值创建一个分支，并在训练集中找到采用该分支的实例。请注意，每个分支都用类节点的一个新实例表示，该实例还包含下一个节点。

对于下一个节点，它计算剩余特征和实例的信息增益，并选择最大化它的那个进行另一次分裂，直到所有实例都具有相同的类或没有更多的特征要计算，返回最可能的类别案件。所有这些都在函数的最后一部分完成：

~~~python
# else...
     # choose the feature that maximizes the information gain
     best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)
     node.value = best_feature_name
     node.childs = []
     # value of the chosen feature for each instance
     feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))
     # loop through all the values
     for value in feature_values:
         child = Node()
         child.value = value  # add a branch from the node to each feature value in our feature
         node.childs.append(child)  # append new child node to current node
         child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value] # instances that take the branch
         if not child_x_ids:
             child.next = max(set(labels_in_features), key=labels_in_features.count)
             print('')
         else:
             if feature_ids and best_feature_id in feature_ids:
                 to_remove = feature_ids.index(best_feature_id)
                 feature_ids.pop(to_remove)
             # recursively call the algorithm
             child.next = self._id3_recv(child_x_ids, feature_ids, child.next)
     return node
~~~



# 最终决策树

决策树分类器完整代码

~~~python
class DecisionTreeClassifier:
    """Decision Tree Classifier using ID3 algorithm."""

    def __init__(self, X, feature_names, labels):
        self.X = X
        self.feature_names = feature_names
        self.labels = labels
        self.labelCategories = list(set(labels))
        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]
        self.node = None
        self.entropy = self._get_entropy([x for x in range(len(self.labels))])  # calculates the initial entropy

    def _get_entropy(self, x_ids):
        """ Calculates the entropy.
        Parameters
        __________
        :param x_ids: list, List containing the instances ID's
        __________
        :return: entropy: float, Entropy.
        """
        # sorted labels by instance id
        labels = [self.labels[i] for i in x_ids]
        # count number of instances of each category
        label_count = [labels.count(x) for x in self.labelCategories]
        # calculate the entropy for each category and sum them
        entropy = sum([-count / len(x_ids) * math.log(count / len(x_ids), 2) if count else 0 for count in label_count])
        return entropy

    def _get_information_gain(self, x_ids, feature_id):
        """Calculates the information gain for a given feature based on its entropy and the total entropy of the system.
        Parameters
        __________
        :param x_ids: list, List containing the instances ID's
        :param feature_id: int, feature ID
        __________
        :return: info_gain: float, the information gain for a given feature.
        """
        # calculate total entropy
        info_gain = self._get_entropy(x_ids)
        # store in a list all the values of the chosen feature
        x_features = [self.X[x][feature_id] for x in x_ids]
        # get unique values
        feature_vals = list(set(x_features))
        # get frequency of each value
        feature_vals_count = [x_features.count(x) for x in feature_vals]
        # get the feature values ids
        feature_vals_id = [
            [x_ids[i]
            for i, x in enumerate(x_features)
            if x == y]
            for y in feature_vals
        ]

        # compute the information gain with the chosen feature
        info_gain = info_gain - sum([val_counts / len(x_ids) * self._get_entropy(val_ids)
                                     for val_counts, val_ids in zip(feature_vals_count, feature_vals_id)])

        return info_gain

    def _get_feature_max_information_gain(self, x_ids, feature_ids):
        """Finds the attribute/feature that maximizes the information gain.
        Parameters
        __________
        :param x_ids: list, List containing the samples ID's
        :param feature_ids: list, List containing the feature ID's
        __________
        :returns: string and int, feature and feature id of the feature that maximizes the information gain
        """
        # get the entropy for each feature
        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]
        # find the feature that maximises the information gain
        max_id = feature_ids[features_entropy.index(max(features_entropy))]

        return self.feature_names[max_id], max_id

    def id3(self):
        """Initializes ID3 algorithm to build a Decision Tree Classifier.
        :return: None
        """
        x_ids = [x for x in range(len(self.X))]
        feature_ids = [x for x in range(len(self.feature_names))]
        self.node = self._id3_recv(x_ids, feature_ids, self.node)
        print('')

    def _id3_recv(self, x_ids, feature_ids, node):
        """ID3 algorithm. It is called recursively until some criteria is met.
        Parameters
        __________
        :param x_ids: list, list containing the samples ID's
        :param feature_ids: list, List containing the feature ID's
        :param node: object, An instance of the class Nodes
        __________
        :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree
        """
        if not node:
            node = Node()  # initialize nodes
        # sorted labels by instance id
        labels_in_features = [self.labels[x] for x in x_ids]
        # if all the example have the same class (pure node), return node
        if len(set(labels_in_features)) == 1:
            node.value = self.labels[x_ids[0]]
            return node
        # if there are not more feature to compute, return node with the most probable class
        if len(feature_ids) == 0:
            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode
            return node
        # else...
        # choose the feature that maximizes the information gain
        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)
        node.value = best_feature_name
        node.childs = []
        # value of the chosen feature for each instance
        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))
        # loop through all the values
        for value in feature_values:
            child = Node()
            child.value = value  # add a branch from the node to each feature value in our feature
            node.childs.append(child)  # append new child node to current node
            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]
            if not child_x_ids:
                child.next = max(set(labels_in_features), key=labels_in_features.count)
                print('')
            else:
                if feature_ids and best_feature_id in feature_ids:
                    to_remove = feature_ids.index(best_feature_id)
                    feature_ids.pop(to_remove)
                # recursively call the algorithm
                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)
        return node

    def printTree(self):
        if not self.node:
            return
        nodes = deque()
        nodes.append(self.node)
        while len(nodes) > 0:
            node = nodes.popleft()
            print(node.value)
            if node.childs:
                for child in node.childs:
                    print('({})'.format(child.value))
                    nodes.append(child.next)
            elif node.next:
                print(node.next)
~~~

