# 机器学习—数据清理

### 1. 数据清理

数据科学家花费大量时间进行数据清理，以确保他们的数据是有效的。

数据清理的步骤主要包括：

* 删除不必要的列 DataFrame；
* 改变索引 DataFrame；
* 使用 .str() 方法清洁色谱柱；
* 使用 DataFrame.applymap() 函数来逐个元素地清理整个数据集；
* 将列重命名为一组更易于识别的标签；
* 跳过 CSV 文件中不必要的行。

#### 1.0 原始数据集分析

这里提供示范数据集 [house_price](https://github.com/DDtime123/Machine-learning-second/blob/master/house_prices.csv) ：

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028231608.png)

该数据集有 7 列，其中数值型的列有 5 列：（house_id，area，bedrooms，bathrooms，price） ；非数值型列有两个：（neighborhood，style）。



#### 1.1 引入必要的库

这里使用 pandas 和 numpy 库进行数据清洗：

~~~python
import pandas as pd
import numpy as np
~~~

#### 1.2 在数据集中删除列

首先查看数据集的各个列：

~~~python
df = pd.read_csv("house_prices.csv")
df.head()
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211025201549.png)

并非所有数据文件中的数据类别都包含在数据集中，pandas 提供给我们简单的删除数据列的方法 DataFrame.drop() 。

其中 price 列是我们的因变量，将它剔除，neighborhood 没有意义，剔除：

~~~python
df.drop(['price','neighborhood'],inplace=True,axis=1)
df.head()
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211025203229.png)

在 drop 方法里，第一个参数定义了要删除的列的集合 [列1，列2...]，参数 inplace=True 是让方法直接在集合上进行修改，而非返回一个新集合，axis =1 则表明在列中寻找删除的对象。

也可以将 ['price'] 和 axis=1 修改为 columns=[列1，列2...] 来直接表明删除的就是列：

~~~python
df.drop(columns=['price'],inplace=True)
~~~



#### 1.3 更改一个索引

pandas 扩展了 numpy 库的数组索引，以实现更为通用的功能和标记。

使用 pandas 读入数据集时，会根据记录的顺序给它们安排索引，而有的时候我们想要使用每条记录的唯一值作为索引。

~~~python
df['house_id'].is_unique # 查看该列是否为唯一值
df.set_index('house_id',inplace=True) # pandas 重新设置索引
df.head()
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211025211510.png)

使用 df.loc[索引] 直接访问每一条记录：

~~~python
df.loc[1112]
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211025211526.png)

#### 1.4 整理数据中的字段

数据集中，有的字段使用文字来表示数据，比如说 地点Location。现在我们将清理这些列，并使它们采用统一格式。

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211026135124.png)

~~~python
df.dtypes.value_counts() # 查看 DataFrame 的列的数据类型
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211026131918.png)

这里有一个 Object 类型的列 style ，4 个 int64 类型的列。

这里我们想要将 style 列的值转化为唯一标识的 int id 值。使用 pandas.factorize() 方法。

pandas.factorize() 方法通过识别不同的值来帮助获取数组的不同表示。

~~~python
# sorting the numerics
label1, unique1 = pd.factorize(df['style'])
  
print("\n\nNumeric Representation : \n", label1)
print("Unique Values : \n", unique1)
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028114228.png)

pandas.factorize() 方法的参数：

* [list]：一维数组
* sort：布尔值，默认是 False，排序的唯一性和随机标签
* na_sentinel：整数值，默认是 -1，当值是空的时的值。

但是，将非数值变量转化为数值变量会在该变量的不同值之间造成数值大小的差别，比如上述的 style 的不同值之间不应该有大小的差别，这不是我们想要的，这时我们要用到虚拟变量。

##### 1.4.1 虚拟变量

在上述的为非数值特征编号过程中，style 特征的各个值被标记为 0,1,2,3... ，但是回归模型认为一个值高于或低于另一个值，这在现实中不正确。

为了解决这种情况，这里引入一个虚拟变量的概念。虚拟变量取值 0 或 1，任意两个不同的值之间只有互斥（0 和 1）的关系。这种关系通过一个行列都是值的个数的 n * n 矩阵来构建。

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028162609.png)

这要用到 sklearn.preprocessing 包中的 OneHotEncoder 库 或者 pandas 的 get_dummies 函数。

~~~python
# pandas
dummy = pd.get_dummies(df['style'],prefix="style") # 将 style 特征构建为虚拟变量
dummy.head()
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028235054.png)

这里 style 有三个唯一值，形成了三个列。



##### 1.4.2 合并虚拟变量和原始的特征变量

虚拟变量与原始数据集没有相同的列，所以可以将它们连接起来：

~~~python
df = pd.concat([df,dummy],axis=1); # 将它们连接到轴1（横轴）上，
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028205810.png)

##### 1.4.3 删除多余的列

因为已经将 style 的值提取出来形成了列，就将原来的 style 列删除掉吧：

~~~python
df.drop(['style'],inplace=True,axis=1);
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028205842.png)

##### 1.4.4 在回归分析中使用虚拟变量

好的，现在我们就可以在回归分析中使用虚拟变量了。

虚拟变量的意义就是将类别特征加入了回归分析中；

如果说 area 在回归方程中的意义是： area 的大小会对回归方程造成影响。

那么使用虚拟变量的类别特征 style 的各个值的意义就是：style 是 ranch ，style 是 lodge 以及 style 是 victorian 三种情况对回归方程造成的影响。

##### 1.4.5 警惕虚拟变量陷阱

这里我用本数据集的一个案例来展示什么是虚拟变量陷阱。

虚拟变量陷阱指的是：构建的虚拟变量之间存在相关关系的情况。比如说当虚拟变量是性别的两个特征（男，女）的时候，因为在实际情况中非男即女，这两个特征互斥的同时又构成全集，这时候将其中一个虚拟变量删除掉就不会对线性回归模型造成任何影响：

这里我删除掉 style_victorian 列，因为 style_victorian 和 style_lodge 和 style_ranch 三者互斥，且共同构成全集  ，所以删除掉它不会对线性模型造成影响。

删除前：

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211029130945.png)

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211029131055.png)

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211029131117.png)

删除后：

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211029130922.png)

可见删除掉 style_victorian  后 r2，均方误差数据都没有变。

而因为使用虚拟变量来构建分类变量时，如果变量的值的数量大于等于二，则一定会造成至少一个完全的线性依赖关系——即 {任意一个特征} 和 {任何其他特征} 之间的线性依赖关系。

为了清除这种依赖关系，我们一般在使用 pandas.get_dummies() 函数构建虚拟变量时直接在参数列表中加入以下参数，表示删除掉第一个虚拟变量（如果多于两个值）：

~~~python
# 加入 drop_first=True
pd.get_dummies(list,drop_first=True)
~~~



### 2. 特征共线性

#### 2.1 共线性简介

多重共线性（也称共线性）是回归模型中的一个特征变量与另一个特征变量高度线性相关的现象。

如果存在两个或多个变量具有共线性，那么就意味着回归系数不是唯一确定的，它会损害模型的可解释性。因为一个变量的回归系数不是唯一的，且会受到其它变量的影响。

因为回归系数的解释是：当所有其他自变量保持不变时，当前自变量 $x_i$ 每变化一个单位时，因变量 $y$ 的平均变化。而我们希望的是当改变一个自变量的值时，不影响其它变量。

#### 2.2 共线性的类型

共线性具有两种基本类型：

* 结构多重共线性：当我们从数据本身而不是实际采样的数据创建新特征时会发生此种共线性。例如，当对一个变量进行平方或对某些变量应用一些算术来创建新变量时，新变量和原始变量之间将存在某种相关性。
* 数据多重共线性：这是我们看到的最常见的共线性。就是在抽样之后观察变量，可以看到部分变量之间在某种程度上相互关联。

#### 2.3 处理共线性的方法

##### 2.3.1 使用 VIF 检测共线性

在生态学中，当相关性大于 0.7 时，计算所有自变量之间的相关矩阵且删除其中一个是很常见的情况，这里可以选择计算每个变量的 “方差膨胀因子” VIF。

VIF 决定了自变量之间相关性的强度，它是通过取一个变量并将其它所有变量进行回归来预测的。

VIF 计算简单易懂：值越高，共线性越高。

为每个解释变量计算一个 VIF，并删除那些具有高值的变量。 “高”的定义有些随意，但通常使用 5-10 范围内的值。它采用以下公式： 

​												$VIF = \frac{1}{1-R^2}$

确定 $R^2$ 的值以找出 VIF ，$R^2$ 值越高，意味着该变量和其它变量相关性越强；$R^2$ 越接近 1 ，VIF 值越高。

~~~python
# Import library for VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)
~~~

VIF 的特点如下：

* VIF 从 1 开始，没有上限
* VIF = 1，自变量与其他变量无相关性
* VIF 超过 5 或 10 表示该自变量和其它变量之间存在高度多重共线性。

~~~python
X = df.iloc[:,:]
calc_vif(X)
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028210738.png)

此处 bedrooms，bathrooms ，victorian 都具有高度共线性，必须进行处理。

##### 2.3.2 开始修复共线性

修复共线性的方法之一是删除相关性高的一对特征中的一个，使用 pandas.DataFrame.drop 方法删除列。

首先删除 area 列，因为面积明显和房间数量（bedrooms，bathrooms）成相关关系，这个特征 VIF 值特别高：

~~~python
df.drop(columns=['area'],inplace=True)
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028221007.png)

删除掉相关特征 area 后， 剩余特征的 VIF 值都下降了。

再删除相关性最高的 bedrooms 列，剩余特征的相关性就少了许多：

~~~python
df.drop(columns=['bedrooms'],inplace=True)
~~~

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211028221058.png)

好的，现在我们的数据以及清理完毕了，可以开始进行线性回归了。

### 3. 参考

[1] [使用 Python 和 Sklearn 的多元线性回归](https://www.analyticsvidhya.com/blog/2021/05/multiple-linear-regression-using-python-and-scikit-learn/)

[2] [使用 Pandas 和 Numpy 进行数据清理](https://realpython.com/python-data-cleaning-numpy-pandas/)