# 机器学习—决策树

决策树的术语：

* 根节点——决策树的根节点，从这个节点开始，种群根据各种特征开始划分
* 决策节点——分裂根节点后得到的节点称为决策节点
* 叶节点——无法进一步分裂的节点叫做叶节点或终端节点
* 子树——决策树的一部分，还能形成一颗树
* 剪枝——减少一些节点以防止过度拟合

### 1. 决策树简介

在机器学习中，分类是一个两步过程，学习步骤和预测步骤。在学习步骤中，模型时基于给定的训练数据开发的。在预测步骤中，模型用于预测给定数据的响应。决策树是最容易理解和解释的分类算法之一。

### 2. 决策树算法

决策树算法属于监督学习算法，可以解决回归和分类问题。

使用决策树的目标是创建一个训练模型，该模型可以通过学习从先验数据（训练数据）推断出的简单决策来预测目标变量的类别或值。

在决策树中，为了预测记录的类标签，这里树的根开始，将节点属性的值与记录的属性进行比较。在比较的基础上，我们沿着那个值的分支跳转到下一个节点。

### 3. 决策树的类型

决策树的类型基于我们拥有的目标变量的类型，它具有两种类型：

* 分类变量决策树：具有分类变量的决策树叫做分类变量决策树。
* 连续变量决策树：决策树有一个连续的目标变量，成为连续变量决策树。

假设现在有一个保险公司要预测一个客户（是/否）会续订保费，它在进行分类变量预测。而在这一过程中，保险公司要通过预测客户的连续收入来确定它是否会续保，那么这就是在进行连续变量预测。

### 4. 创建决策树时的假设

以下是我们在使用决策树时所做的一些假设：

* 一开始，整个训练集被认为是根
* 特征值最好是分类的，如果这些值时连续的，则在构建模型之前将它们离散化
* 记录根据特征值递归分布
* 将属性作为树的根节点或内部节点的顺序是通过使用某种统计方法完成的。n决策树遵循产品总和（SOP）表示，产品总和（SOP）也成为析取范式。对于一个类别，从书的根到具有相同类别的叶节点的每个分支都是值的合取（乘积），也该类结尾的不同分支形成析取（和）。

决策树实现中的主要挑战是确定我们需要考虑哪些属性作为树的根节点和每个级别。处理这个问题就是我们的属性选择过程。我们有不同的属性选择措施来识别每个级别可作为决策节点的属性。

### 5. 决策时如何工作

决策树使用多种算法来决定将一个界定按拆分为两个或多个子节点。子节点的创建增加了所得子节点的同质性。

决策树中使用的一些算法如下：

* ID3—（D3的扩展）
* C4.5—（ID3的后继者）
* CART—（分类和回归树）
* CHAID—（卡方自动交互检测计算分类树时执行多级分裂）
* MARS—（多元自适应回归样条）

ID3算法使用自上而下的贪婪搜索方法可能通过可能的分支空间构建决策树，而没有回溯。贪心算法总是做出当前似乎做好的选择。

#### 5.1 ID3算法步骤：

1.它以原始集合S作为根节点开始。

2.在算法的每次迭代中，它遍历集合S中未使用的属性并计算该属性的熵(H)和信息增益(IG)。

3.然后它选择具有最小熵或最大信息增益的属性。

4.然后，集合S被选定的属性拆分以生成数据的子集。

5.该算法继续在每个子集上重复出现，只考虑从前为选择过的属性。

### 6. 属性选择方法

如果数据集由N个属性构成，那么决定将哪个属性放置在树的根或不同级别作为内部节点是一个复杂的步骤。仅仅随机选择人任何节点作为根节点都不能解决问题。如果我们遵循随即方法，它可能会给我们带来低准确度的糟糕结果。

为了解决这个属性选择问题，研究人员设计了一些解决方案，他们建议使用一些标准：

* 熵
* 信息增益
* 基尼指数
* 增益比
* 卡方检验

这些标准将计算每个属性的值，对值进行排序，并按照顺序将属性放置在树中，即具有高值（在信息增益的情况下）的属性放置在根处。

在使用信息增益作为标准时，我们假设属性是分类的，对于基尼指数，假设属性是连续的。

### 7. 熵

熵时数据集中的不确定性或无序度量，表示数据的随机性。熵越高，从该信息中得出任何结论就越困难。以下是抛硬币的例子：

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211031134715.png)

当概率为 0 或 1 时，熵 H(X) 为 0 ，当概率为 0.5 时，熵最大，因为现在数据呈现完美的随机性。

ID3 遵循规则——熵为0的分支是叶节点，熵大于0的分支需要进一步分裂。

单个属性的熵的数学表示为：

​										$E(S) = \sum^c_{i=1}-p_ilog_2p_i$

其中 S 表示当前状态， c 表示属性的类别的数量，$p_i$ 表示某个事件在属性的所有事件中占的比例。

比如：

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211031141116.png)

其中属性 Play Golf 一共有两个事件（Yes，No），其中 Yes 事件占的比例为 9/(9+5) = 0.64，No 事件占的比例为 0.36 。

多个属性的数学熵表示为：

​									$E(T,X)=\sum_{c\in{X}}P(c)E(c)$

比如：

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211031144918.png)



其中 T 表示当前状态，X 表示选定属性。

### 8. 信息增益

信息增益 IG 是一种统计属性，用于衡量给定属性根据其目标分类将训练示例分离的程度。构建决策树就是为了寻找一个返回最高信息增益和最小熵的属性。

信息增益就是熵的减少。它根据给定的属性值计算数据集拆分前的熵和拆分后的熵之间的平均差异。ID3决策树算法使用信息增益。

在数学上，IG 表示为：

​								$IG(T,X) = E(T)-E(T,X)$

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211031152849.png)

数据集拆分后的 Sunny 属性的信息增益为 ：PlayGolf 的信息熵 E(PlayGolf) 减去 拆分成后的平均熵 P(Sunny)*E(PlayGolf,Outlook=Sunny)。

### 9. 基尼指数

基尼指数（Gini）可以理解为用于评估数据集中拆分的成本函数。它是通过从 1 中减去每个类的概率平方和来计算的。它有利于较大的分区并且易于实现，而信息增益有利于具有不通知的较小分区。

以下是基尼指数公式：

​											$Gini=1-\sum^c_{i=1}(pi)^2$

基尼指数与目标分类变量 “成功” 或 “失败” 一起使用，它只支持二元拆分。

基尼指数越高，意味着不平等程度越高，异质性越高。

计算拆分基尼指数的步骤：

1.计算子节点的基尼指数，实验用上面的成功（p）和失败（q）的公式 $p^2+q^2$。

2.使用分裂的每个节点的加权基尼分数计算分裂的基尼指数。



### 10. 避免决策树中的过拟合

决策树的常见问题就是过拟合。通常在过拟合中，决策树模型会对训练集给出非常高的拟合度，因为理论上决策树可以通过为每个观察结果制作一个叶子而达成 100% 的准确度，但是这样的模型并不能很好地预测未知的数据。

有两种方法可以防止过拟合：

1.剪枝

2.使用随机森林方法

#### 10.1 剪枝

分裂过程使得树不停增长，直到达到停止标准。但是树的生长可能会过度拟合数据，导致对预测未知数据的准确度较低。

在剪枝过程中，我们从叶节点开始删除决策节点，从而不影响整体准确性。这是通过将实际数据集分为两组来完成的。数据集被分为训练集 D 和测试集 V。使用分离的训练集 D 准备决策时，然后继续相应的修剪树以优化决策树在测试集 V 上的的准确性。

![](https://gitee.com/zhang-jianhua1/blogimage/raw/master/img/20211031165054.png)

右图是修剪过后的决策树， age 属性已经被修剪，因为与 age 从同一个父节点分裂出来的 specacle prescription 属性的重要性更高，所以可以剪除 age 。

#### 10.2 随机森林

随机森林是集成学习的一个例子，其中我们结合了多种机器学习算法以获得更好地预测性能。

随机的意义：

1.构建树时，训练集从数据集中随机抽样获得。

2.分割节点时考虑的随机特征子集。

一种称为装袋的技术用于创建树的集合，其中生成多个训练集并替换。

在装袋技术中，使用随机抽样将数据集划分为 N 个样本，然后，使用算法在所有样本上建立模型，使用投票或者平均并行方法合并得到的预测。

### 11. 线性模型和基于树的模型的比较

根据要解决的问题类型，

1.因变量和自变量的关系被线性地模型近似，线性回归会优于基于树的模型。

2.因变量和自变量之间存在高度非线性和复杂的关系，树模型将优于经典回归模型。

3.如果需要建立一个易于向人们解释的模型，决策树模型总是比线性模型更好。